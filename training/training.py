import numpy as np
import tensorflow as tf
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import Adam
from keras.utils import to_categorical, set_random_seed

if __name__ == "__main__":
    print("Tensorflow Version: " + tf.__version__)
    # Set random seed for reproducible results
    set_random_seed(42) # Where's my towel?

    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    # Compute the number of labels
    num_labels = len(np.unique(y_train))
    if num_labels != 10:
        raise Exception("Invalid number of dataset labels")

    # tf.image.resize requires a 4D Tensor with channel dimension, we add it here to the images
    x_train = np.expand_dims(x_train, axis=-1)
    x_test = np.expand_dims(x_test, axis=-1)

    # Resize images from 28x28 to 7x7
    x_train = tf.image.resize(x_train, [7, 7]).numpy()
    x_test = tf.image.resize(x_test, [7, 7]).numpy()

    # Flatten (convert image data to 1D) the image
    # Normalize image pixels (from range [0,255] to range [0,1])
    input_size = 7*7
    x_train = np.reshape(x_train, [-1, input_size])
    x_train = x_train.astype('float32') / 255
    x_test = np.reshape(x_test, [-1, input_size])
    x_test = x_test.astype('float32') / 255

    # Convert labeled values to one-hot vector
    # e.g. 2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
    y_train = to_categorical(y_train, num_classes=num_labels)
    y_test = to_categorical(y_test, num_classes=num_labels)

    # Define model with Keras
    dropout_rate = 0.4
    hidden_layer_size = 24
    batch_size = 64

    model = Sequential()
    model.add(Dense(input_size, input_dim=input_size))
    model.add(Activation("relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(hidden_layer_size))
    model.add(Activation("relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(num_labels))
    model.add(Activation("softmax"))

    model.summary()

    model.compile(
        loss='categorical_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )
    model.fit(x_train, y_train, epochs=50, batch_size=batch_size)

    loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
    print("Test accuracy: %.1f%%" % (100.0 * acc))

    print("Exporting weights")

    weights = []
    for layer in model.layers:
        weights_list = layer.get_weights()

        # Skip empty lists (e.g. Activation and Dropout layers have no weights)
        if not weights_list:
            continue

        # In a Dense layer, get_weights() returns [kernel, bias]
        kernel, bias = weights_list[0], weights_list[1]
        num_inputs, num_neurons = kernel.shape

        # Export data with this format: [bias, weight1, weight2, ...], for each neuron,
        # to make it compatible with the C code
        for neuron_index in range(num_neurons):
            weights.append(bias[neuron_index])
            for input_index in range(num_inputs):
                weight = kernel[input_index, neuron_index]
                weights.append(weight)

    print("Exporting weights to model_weights.txt file...")
    with open("model_weights.txt", "w") as f:
        for weight in weights:
            f.write(f"{weight}\n")
    print("Weights saved in file model_weights.txt.")

    print("Generating weights.h file...")
    with open("weights.h", "w") as f:
        # Generate C header file from a template
        f.write("#ifndef NES_AI_WEIGHTS_H\n#define NES_AI_WEIGHTS_H\n\n")
        f.write("#include <fix16.h>\n\n")
        f.write("// THIS FILE HAS BEEN GENERATED BY THE TRAINING.PY SCRIPT, DON'T MODIFY IT!\n")
        f.write(f"#define MLP_LAYER1_INPUT_SIZE {input_size}\n")
        f.write(f"#define MLP_LAYER1_OUTPUT_SIZE {input_size}\n")
        f.write(f"#define MLP_LAYER2_OUTPUT_SIZE {hidden_layer_size}\n")
        f.write(f"#define MLP_OUTPUT_SIZE {num_labels}\n\n")
        f.write("extern const fix16_t WEIGHTS[];\n\n")
        f.write("#endif // NES_AI_WEIGHTS_H\n")

    print("Exporting weights to weights.c file...")
    with open("weights.c", "w") as f:
        # Generate C source file from a template
        f.write("#include <fix16.h>\n\n")
        f.write("#include \"weights.h\"\n\n")
        f.write("// THIS FILE HAS BEEN GENERATED BY THE TRAINING.PY SCRIPT, DON'T MODIFY IT!\n")
        f.write("const fix16_t WEIGHTS[] = {\n")

        # Convert weight to an uint32 Q16.16 HEX value, equivalent to F16() macro.
        # We do it here because cc65 doesn't support floating point operations (even in the preprocessing phase).
        # Resulting floating point is less accurate: -0.12657789885997772 (float64) in Q16.16 becomes FFFFDF99, and if
        # we convert it back to a float64, its value is -0.126572
        weights_uint = []
        for weight in weights:
            if weight >= 0:
                weight = weight * 65536.0 + 0.5
            else:
                weight = weight * 65536.0 - 0.5
            weight_uint = np.float64(weight).astype(np.uint32)
            weights_uint.append(weight_uint)

        weights_str = ",\n".join(f"    0x{weight_uint:08X}" for weight_uint in weights_uint)
        f.write(weights_str)
        f.write("\n")

        f.write("};\n")
    print("Weights saved in file weights.c.")
